# Rainfall Forecast Validation Demo

## Project Purpose

This project demonstrates a Python-based system for validating rainfall forecasts against observed rainfall data. It processes raster-based forecast and observation files, calculates area-based statistics using a shapefile to define zones of interest, and generates validation metrics such as Root Mean Squared Error (RMSE), Bias (Mean Error), and Pearson Correlation Coefficient (R).

The primary goal is to provide a clear and configurable example of how such a validation workflow can be implemented.

## Features

*   Processes rainfall data in common raster formats (e.g., GeoTIFF).
*   Uses a shapefile to define polygonal areas for zonal statistics.
*   Calculates key continuous validation metrics: RMSE, Bias, and Pearson R.
*   Highly configurable through an external JSON file (`config_demo.json`).
*   Supports multiple forecast models, initialization periods, and lead times.
*   Generates sample forecast, observation, and boundary data for easy testing.
*   Outputs detailed per-area validation results to a CSV file.
*   Outputs aggregated metrics per model/lead time to a CSV file.
*   Generates a summary Excel report of aggregated metrics.
*   Includes logging for monitoring the validation process.

## Project Structure

```
RainfallForecastValidation_Demo/
├── config_demo.json            # Configuration file for the validation process
├── data/                       # Input data
│   ├── forecast_generic_model_A/ # Sample forecast data for Model A
│   ├── forecast_generic_model_B/ # Sample forecast data for Model B
│   ├── observed_rainfall/        # Sample observed rainfall data
│   └── shapefiles_demo/          # Sample boundary shapefile
├── output/                     # Output files generated by the validation
│   ├── demo_processing_log.txt
│   ├── demo_rainfall_means_and_metrics.csv
│   ├── demo_rainfall_validation_results.csv
│   └── demo_validation_matrix.xlsx
├── scripts/                    # Python scripts
│   ├── demo_validation_processor.py # Core logic for validation processing
│   ├── generate_sample_data.py    # Script to create sample input data
│   └── run_demo_validation.py     # Runner script to execute the validation
└── README.md                   # This file
```

## Prerequisites

*   Python (version 3.8 or higher recommended)
*   The Python packages listed in `requirements.txt`.

## Setup Instructions

1.  **Clone the Repository:**
    ```bash
    # If this project is on GitHub, clone it:
    # git clone <repository_url>
    # cd RainfallForecastValidation_Demo
    ```
    (If you have downloaded it as a ZIP, extract it and navigate to the `RainfallForecastValidation_Demo` directory.)

2.  **Install Dependencies:**
    It's highly recommended to use a virtual environment.
    ```bash
    python -m venv .venv
    # Activate the virtual environment
    # On Windows:
    # .venv\Scripts\activate
    # On macOS/Linux:
    # source .venv/bin/activate
    ```
    Install the required packages:
    ```bash
    pip install -r requirements.txt
    ```
    *(Note: The `requirements.txt` file will need to be created. See "Next Steps" below.)*

## How to Run

This demo project comes with pre-generated sample data located in the `data/` directory.

1.  **Run the Validation Process:**
    Navigate to the `RainfallForecastValidation_Demo` directory if you are not already there.
    To run the validation using the pre-existing sample data and the default configuration:
    ```bash
    python scripts/run_demo_validation.py
    ```
    The script will process the data and generate output files in the `output/` directory.

    You can also filter for specific models or initialization months using command-line arguments:
    ```bash
    # Example: Process only model_A for initialization month January (1)
    python scripts/run_demo_validation.py --models model_A --init_months 1
    ```

2.  **(Optional) Regenerating Sample Data:**
    The script `scripts/generate_sample_data.py` is used to create the sample input data (raster files and shapefile) found in the `data/` directory. You typically do not need to run this script unless you have modified or deleted the sample data, or if you are interested in seeing how it's generated.
    To regenerate all sample data (this will overwrite existing sample data):
    ```bash
    python scripts/generate_sample_data.py
    ```

## Configuration (`config_demo.json`)

The `config_demo.json` file controls various aspects of the validation process:

*   `base_path`: (Usually ".") Specifies the root directory for resolving relative paths within the config (e.g., data and output folders).
*   `shapefile_config`: Defines the path to the boundary shapefile and attribute columns for area ID and name.
*   `default_crs`: The default Coordinate Reference System (e.g., "EPSG:4326") to use if reprojection is needed.
*   `raster_processing_config`: Contains settings like `nodata_value` and `resampling_method`.
*   `iteration_parameters`: Specifies the years, initialization months, and lead times to process.
*   `models_config`: An array defining each forecast model, including its name, path to forecast data, and filename patterns.
*   `observed_config`: Defines the path to observed data and its filename pattern.
*   `output_config`: Specifies the output folder and names for the generated CSV and Excel files.
*   `logging_config`: Configures the log file name and logging level.

## Output Files

The validation process generates the following files in the `output/` directory:

*   `demo_rainfall_validation_results.csv`: Contains detailed per-area validation metrics for every processed forecast, including forecast value, observed value, error, and squared error.
*   `demo_rainfall_means_and_metrics.csv`: Provides aggregated metrics (RMSE, Bias, Pearson R, and N_Pairs) for each model and lead time combination.
*   `demo_validation_matrix.xlsx`: An Excel file presenting the aggregated metrics from `demo_rainfall_means_and_metrics.csv` in a matrix format (models vs. lead times) for easier comparison.
*   `demo_processing_log.txt`: A log file detailing the steps, warnings, and errors encountered during the validation process.
*   `plot_rmse_vs_lead_time.png`: A line plot showing RMSE vs. Lead Time for each forecast model.
*   `plot_mean_error_bias_vs_lead_time.png`: A line plot showing Bias (Mean Error) vs. Lead Time for each forecast model.
*   `plot_pearson_r_vs_lead_time.png`: A line plot showing Pearson Correlation (R) vs. Lead Time for each forecast model.
*   `plot_validation_areas.png`: A map displaying the validation area polygons used for the analysis.

---

*This README provides a general overview. You may need to adjust paths or commands based on your specific environment if you deviate from the provided structure.* 